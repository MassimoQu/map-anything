# OPV2V Image-Only 阶段进展（无初值协同感知）

## 1. 当前工作位置
- 课题目标是把多车协同感知摆脱对高精度初值的依赖：每辆车只输出多视角 RGB，由 3D 基础模型恢复自身点云和姿态，再完成跨车融合。
- 前一阶段已经确定了两条支撑链路：  
  - **Stage1**：只面向主车 4 个摄像头，确保单车可在纯图像输入下恢复精确几何。  
  - **Stage2**：引入其他车辆的视角，尝试在没有真实外参的情况下完成对齐。
- 本次重点是补上“按部署要求评估”的缺口：我们让推理端只看 RGB + 内参，借此确认现有模型在真实限制下的误差水平，为后续协同策略提供真实参考。

## 2. 实验设置
1. **样本**：OPV2V test split 中 10 个时间戳（均包含 ≥2 车辆），帧列表固定在 `summary_test.json`，保证单车/协同都能在同一批帧对比。
2. **环境**：`mapanything_reconstructed`（Torch 2.6 / CUDA 12.4）。通过 GPU 1/4/5 并行运行 `scripts/batch_eval.py`，一次性评估 pretrain、Stage1、Stage2。
3. **命令要点**：  
   ```bash
   CUDA_VISIBLE_DEVICES={1,4,5} python scripts/batch_eval.py \
     --split test \
     --frames_json .../summary_test.json \
     --output_root /media/.../opv2v_batch_eval_colorfix \
     --device cuda --save_representative \
     --pc_metrics --pc_filter_z_min -1 --pc_filter_z_max 4.2 --pc_filter_radius 120 \
     --model_filter {pretrain|stage1|stage2}
   ```
4. **输入改动**：评估前在数据管线中调用 `strip_external_calibration_inputs` 并移除 `depth_z`，推理阶段只能看到 RGB + intrinsics；真值深度仅在计算指标时参与。
5. **输出资产**：`/media/.../opv2v_batch_eval_colorfix` 下保存每个模型/模式的指标 CSV、代表性点云和可视化资源，后续扩样时继续复用。

## 3. 主要结果（Image-Only）

### 3.1 单车模式

| 模型 | Pose Abs (m) | Pose Rot (°) | Depth RMSE | Depth MAE | Depth Rel | Scale Err |
| --- | --- | --- | --- | --- | --- | --- |
| Pretrain | 6.35 | 18.59 | 15.19 | 6.99 | 0.38 | 10.31 |
| Stage1 | **0.058** | **0.62** | **14.10** | **5.47** | **0.23** | **3.32** |
| Stage2 | 0.16 | 1.02 | 14.87 | 6.71 | 0.35 | 16.59 |

- Stage1 在纯图像输入下仍保持 5–6 cm 精度，成为可信的单车基线。
- Stage2 在缺少协同视角时尺度误差飙升（16.6），表明其 metric scaling 依赖多车信息。
- 预训练模型几乎无法依靠图像自恢复姿态（6 m / 18°），不适合作为部署模型。

### 3.2 协同模式

| 模型 | Pose Abs (m) | Pose Rot (°) | Depth RMSE | Depth MAE | Depth Rel | Scale Err |
| --- | --- | --- | --- | --- | --- | --- |
| Pretrain | 6.28 | 4.59 | 15.04 | 7.22 | 0.41 | 11.47 |
| Stage1 | 10.85 | 12.66 | **14.16** | **5.72** | **0.26** | **3.31** |
| Stage2 | **2.09** | **1.50** | 15.07 | 6.94 | 0.38 | 19.01 |

- Stage2 是唯一在协同条件下有积极效果的模型，但仍有 2 m 级平移和 19 级尺度偏差，远未达到实际协同需求。
- Stage1 协同反而恶化（10 m 以上），当前形态只适合单车。

### 3.3 点云 & BEV

| 模型-模式 | Chamferᵣ | Chamferᶠ | BEV IoUᵣ | BEV IoUᶠ |
| --- | --- | --- | --- | --- |
| Pretrain-单车 | 9.64 / 3.18 | 8.08 / 8.71 | 0.145 | 0.025 |
| Stage1-单车 | **5.81 / 2.87** | 7.25 / 7.91 | 0.097 | 0.033 |
| Stage2-单车 | 6.44 / 2.80 | 7.06 / 7.66 | 0.144 | 0.045 |
| Pretrain-协同 | 12.46 / 3.13 | 7.45 / 8.10 | 0.162 | 0.036 |
| Stage1-协同 | **5.51 / 2.79** | 6.94 / 7.43 | 0.112 | 0.050 |
| Stage2-协同 | 11.78 / 2.95 | **6.59 / 7.04** | **0.161** | **0.065** |

- Stage2 协同在 BEV IoUᶠ 上最优 (0.065)，说明多车输入确实帮助把点云集中到检测区域；但 Chamferᵣ 较大，源于整体平移。
- Stage1 单车在低高度点云的 Chamferᶠ 基本对齐地面，可直接供单车检测/定位使用。

## 4. 遇到的问题与定位
- **外参/深度泄漏造成的假指标**：原始脚本默认把真值外参和深度作为模型输入，导致数值远高于实际。通过剥离该部分输入，我们得到与真实部署一致的误差范围。
- **Stage2 尺度不稳定**：`scale_err≈16–19` 说明多车微调阶段没有学到统一尺度。查看 `stage2/coop_metrics.csv:4` 等问题帧时，可以看到推理姿态与真值差距巨大，需在训练策略中专门约束。
- **Stage1 协同“负迁移”**：当单车模型看到其他车辆图像时，其主车姿态会被干扰；在 `stage1/coop_metrics.csv:3` 等案例中，误差高达 23 m，提示后续要把 Stage1 明确限定为单车模块。

## 5. 当前成果
1. 完成了与部署假设一致的评估流程，得到可信的 base numbers：Stage1 单车 ≈5 cm，Stage2 协同 ≈2 m。
2. 将运行脚本、过滤阈值、样本列表固化为 `/media/.../opv2v_batch_eval_colorfix`，为下一批试验提供可复用资产。
3. 确认 Stage2 的协同优势来源于点云覆盖更集中而非姿态精度，后续需要针对尺度和对齐过程继续改造。

## 6. 下一步计划
1. **扩样与统计**：把同样的 image-only 流程扩大到 ≥20 帧，记录均值/方差，形成长期基准。
2. **Stage2 尺度诊断**：挑选误差最大的帧做逐相机特征对齐、对比推理/真值姿态，判断是否需要在训练中引入尺度蒸馏或外参自校正模块。
3. **训练配置对齐**：在 Stage2 训练脚本中默认禁用外参输入，并尝试把 Stage1 的尺度回归头蒸馏至 Stage2，保持模态统一。
4. **协同融合改造**：结合 `coalign` 等鲁棒配准手段，尝试在推理后阶段纠正 Stage2 的尺度漂移，验证是否能把 2 m 降到可接受范围。

## 7. 待办清单
- 扩大回归样本并持续记录 image-only 指标。
- 梳理 Stage2 推理过程中各摄像头的姿态残差，准备插入尺度监督。
- 评估点云后处理（过滤、配准）对 BEV IoUᶠ 与 Chamfer 的改善幅度，决定是否纳入协同 pipeline。

以上工作直接延续了《20251104 无初值协同感知实验进展》中提出的方案：先把 3D 基础模型在无初值条件下跑通，再逐步解决尺度与协同一致性问题。当前评估结果为后续训练/融合策略提供了真实的误差基线。
